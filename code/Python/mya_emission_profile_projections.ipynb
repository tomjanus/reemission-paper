{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Myanmar Case Study** – Projecting Reservoir Emissions Under Construction Date Uncertainty  \n",
    "#### **Tomasz Janus** – *Manchester, UK, 11/06/2025*\n",
    "\n",
    "This study explores potential trajectories of biogenic greenhouse gas emissions produced in hydroelectric, multipurpose and irrigation reservoirs in Myanmar under uncertainty about their construction dates. It uses known construction dates of existing reservoirs and information contained in existing hydropower expansion plans to estimate plausible construction timelines for reservoirs with missing information about their construction dates.\n",
    "\n",
    "Given the limited availability of information about construction dates  of some reservoirs - particularly irrigation reservoirs and planned hydropower projects - we employ random sampling to generate multiple plausible scenarios of historical and future reservoir developments. For each scenario, we simulate the temporal evolution of biogenic emissions, allowing us to assess the likely magnitude and timing of the emission peak and cumulative emissions between 1965 and 2035 / 2040 (two planning horizons).\n",
    "\n",
    "Reservoirs in Myanmar are categorized into three primary types: *hydroelectric*, *multipurpose*, and *irrigation*. The analysis excludes other types of reservoirs such as *water supply* and *flood control*. Multipurpose reservoirs are assumed to be primarily hydroelectric, with secondary uses including irrigation and potable water supply.\n",
    "\n",
    "Known construction dates are available for most of the existing hydroelectric and multipurpose reservoirs. However, dates are missing for many irrigation reservoirs and for most planned hydroelectric and multipurpose dams. To address this, we apply the following methodology:\n",
    "\n",
    "1. **Existing Irrigation Reservoirs:**  \n",
    "   We calculate a histogram of construction dates using those irrigation reservoirs for which dates are available. This histogram reflects historical trends in irrigation infrastructure investment. We then derive a kernel density estimate (KDE) from which we randomly sample plausible construction years for built irrigation reservoirs with missing construction dates.\n",
    "\n",
    "2. **Planned Hydroelectric and Multipurpose Dams:**  \n",
    "   These reservoirs tend to be better documented, however construction dates of planned projects are often not known. In such cases, we sample construction dates uniformly between the present year (2025) and a plausible horizon for project completion (e.g., 2035 or 2040).\n",
    "\n",
    "Emission trajectories are computed using *Re-emission* software. The results are visualized as ensemble plots, showing a range of potential emission pathways along with the median trajectory, disaggregated by reservoir type (irrigation, multipurpose, and hydroelectric).\n",
    "\n",
    "#### Period\tActivity Highlights - Literature\n",
    "* 1988–1995 - approx. 97 dams/reservoirs built—or nearly a hundred—initiated/commissioned.\n",
    "* Mid-1990s–early 2000s\t- Bulk of large dams commissioned into service (e.g., Thaphanseik 2002; Paunglin 2004).\n",
    "* By ~2010 - Total irrigation-dam count exceeded 200.\n",
    "\n",
    "A breakdown by individual year (e.g., how many were built in 1990 vs. 1995) is impossible to establish as that data isn’t available in public sources. The planning and commissioning clusters clearly centered on 1988–2002.\n",
    "\n",
    "#### NOTE:\n",
    "Here, we sample **1000** possible futures such that we have a statistically relevant sample. On most modern computers, the simulation should execute within minutes. However, on older computers the code may run slowly. For testing purposes, in order to speed up the execution, **reduce the sample size** (e.g. down to 100 samples) and/or **increase number of processes** in concurrent futures. Currently the number is set to 4 as this is the number of CPUs available on the machine on which the calculations have been performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple, Any, Dict, TypeAlias, Union, Optional, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import json\n",
    "import rich\n",
    "import pickle\n",
    "import reemission\n",
    "from reemission.model import EmissionModel\n",
    "from reemission.input import Inputs\n",
    "from reemission.emission_profile import \\\n",
    "    EmissionProfile, EmissionProfileSeries, ureg, Emission\n",
    "\n",
    "class KDEOutput(NamedTuple):\n",
    "    x_d: np.ndarray\n",
    "    density: np.ndarray\n",
    "    kde: KernelDensity\n",
    "    \n",
    "ReservoirName: TypeAlias = str\n",
    "ScenarioName: TypeAlias = str\n",
    "\n",
    "dam_db = Path('../data/dams_data/mya_dams.geojson') # Dam information database\n",
    "input_file = Path('../data/myanmar_inputs.json') # Input data file\n",
    "construction_years = Path('../data/reservoir_data_with_years.csv') # Reservoir tabular data with construction years\n",
    "selected_columns = [\n",
    "    'Project Name', 'Dam Type', 'Function', 'Run-of-River or Storage',\n",
    "    'COD Year']\n",
    "gdf_mya = gpd.read_file(dam_db)\\\n",
    "    .filter(items=selected_columns)\\\n",
    "    .query(\"`Run-of-River or Storage` != 'RoR'\")\n",
    "    \n",
    "# If roughly 97 dams were built between 1988 and 1995 (see the Introduction at the top of this notebook),\n",
    "# we can estimate the average number of dams built per year\n",
    "average_dams_per_year = floor(97 / (1995 - 1988 + 1))\n",
    "print(f\"Average dams built per year: {average_dams_per_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(data: Dict[str, Any], n: int = 5) -> None:\n",
    "    \"\"\"Print the first n items of a dictionary.\"\"\"\n",
    "    rich.print(dict(islice(data.items(), 5)))  # Print the first 5 key-value pairs\n",
    "\n",
    "def save_dict_to_json(data: Dict[str, Any], json_path: str, indent: int = 2) -> None:\n",
    "    \"\"\"Save a dictionary to a JSON file.\"\"\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=indent, ensure_ascii=False)\n",
    "\n",
    "def add_average_counts(df_counts, start_year, end_year, average_count):\n",
    "    # Make a copy and ensure index is integer type (years)\n",
    "    df = df_counts.copy()\n",
    "    df.index = df.index.astype(int)\n",
    "    # Ensure the column is named 'Count'\n",
    "    if df.columns[0] != 'Count':\n",
    "        df.columns = ['Count']\n",
    "    years = list(range(int(start_year), int(end_year) + 1))\n",
    "    for year in years:\n",
    "        if year in df.index:\n",
    "            df.at[year, 'Count'] += average_count\n",
    "        else:\n",
    "            df.loc[year] = average_count\n",
    "    # Sort by year (index)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def add_counts_from_dict(df_counts: pd.DataFrame, year_count_dict: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds or updates counts in df_counts using a dictionary of {year: count}.\n",
    "    If the year exists, adds the value; if not, creates a new row.\n",
    "    \"\"\"\n",
    "    df = df_counts.copy()\n",
    "    df.index = df.index.astype(int)\n",
    "    if df.columns[0] != 'Count':\n",
    "        df.columns = ['Count']\n",
    "    for year, count in year_count_dict.items():\n",
    "        year = int(year)\n",
    "        if year in df.index:\n",
    "            df.at[year, 'Count'] += count\n",
    "        else:\n",
    "            df.loc[year] = count\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "def calculate_kde(count_data: pd.DataFrame, bandwidth=1.0) -> KDEOutput:\n",
    "    \"\"\"Calculate Kernel Density Estimation (KDE) for the given count data.\"\"\"\n",
    "    counts = count_data['Count']\n",
    "    years = count_data.index.astype(int)\n",
    "    # Prepare data for KDE: repeat years according to their count\n",
    "    kde_data = np.repeat(years, counts.astype(int))\n",
    "    kde_data = np.array(kde_data)[:, np.newaxis]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(kde_data)\n",
    "    x_d = np.linspace(years.min(), years.max(), 200)[:, np.newaxis]\n",
    "    log_dens = kde.score_samples(x_d)\n",
    "    return KDEOutput(x_d=x_d, density=np.exp(log_dens), kde=kde)\n",
    "    \n",
    "def sample_years_kernel(\n",
    "        kde: KernelDensity,\n",
    "        n_samples: int, \n",
    "        random_state=None):\n",
    "    \"\"\"\n",
    "    Draw n_samples construction years either from an empirical histogram\n",
    "    or from a pre-fitted KernelDensity estimator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kde : sklearn.neighbors.KernelDensity\n",
    "        If provided, samples are drawn from this KDE; otherwise falls back to empirical PMF.\n",
    "    n_samples : int\n",
    "        Number of unknown dams to assign years to.\n",
    "    random_state : int or numpy.random.Generator, optional\n",
    "        Seed or RNG for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Array of length n_samples of sampled years (dtype=int).\n",
    "    \"\"\"\n",
    "    cont = kde.sample(n_samples, random_state=random_state)\n",
    "    years = np.rint(cont.flatten()).astype(int)\n",
    "    return years\n",
    "\n",
    "def sample_years_uniform(\n",
    "        min_year: int, max_year: int, n_samples: int, random_state=None):\n",
    "    \"\"\"\n",
    "    Draws n_samples years uniformly at random between min(years) and max(years), inclusive.\n",
    "    Returns a numpy array of sampled years (dtype=int).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    sampled_years = rng.integers(min_year, max_year + 1, size=n_samples)\n",
    "    return sampled_years\n",
    "\n",
    "def histogram_plot(count_data: pd.DataFrame, title: str, bandwidth: float = 1.0) -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    counts = count_data['Count']\n",
    "    years = count_data.index.astype(int)\n",
    "    # Bar plot: years vs counts\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(years, counts, color='skyblue', edgecolor='black', label='Counts per Year')\n",
    "    # KDE (Gaussian)\n",
    "    # Only plot KDE if there are at least 2 data points\n",
    "    if len(counts) > 1:\n",
    "        # Prepare data for KDE: repeat years according to their count\n",
    "        kde_data = np.repeat(years, counts.astype(int))\n",
    "        kde_data = np.array(kde_data)[:, np.newaxis]\n",
    "\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(kde_data)\n",
    "        x_d = np.linspace(years.min(), years.max(), 200)[:, np.newaxis]\n",
    "        log_dens = kde.score_samples(x_d)\n",
    "        bin_width = 1  # Each year is a bin\n",
    "        scale = len(kde_data) * bin_width\n",
    "        plt.plot(x_d[:, 0], np.exp(log_dens) * scale, color='red', lw=2, label='Gaussian KDE')\n",
    "        # The scaling factor (/10) is to make the KDE visually comparable to the bar heights; adjust as needed.\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "def histogram_plot_simple(years: List[int], title: str = \"\") -> None:\n",
    "    \"\"\" \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(\n",
    "        years, bins=range(min(years), max(years) + 2), color='skyblue', \n",
    "        edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Probability Distribution from Data to Impute Missing Irrigation Reservoir Construction Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Find known construction years for irrigation reservoirs from the geojson dam database\n",
    "df_counts_irr = gdf_mya\\\n",
    "    .query(\"Function == 'IRR'\")[[\"COD Year\"]]\\\n",
    "    .value_counts()\\\n",
    "    .sort_index()\\\n",
    "    .rename_axis('Year')\\\n",
    "    .reset_index(name='Count')\\\n",
    "    .set_index('Year')\n",
    "# Add average number of dams built per year between 1988 and 1995 from data (mentioned in the Introduction)\n",
    "df_counts_irr = add_average_counts(\n",
    "    df_counts_irr, start_year=1988, end_year=1995, average_count=average_dams_per_year)\n",
    "# Add average number of dams built per year between 1997 and 2010\n",
    "df_counts_irr = add_average_counts(df_counts_irr, 1997, 2010, 1)\n",
    "# Add dam counts from a manually constructed dictionary between years 1979 and 1988\n",
    "year_count_dict = {\n",
    "    1979: 1, \n",
    "    1980: 1, \n",
    "    1981: 2,\n",
    "    1982: 3,\n",
    "    1983: 3,\n",
    "    1984: 4,\n",
    "    1985: 5,\n",
    "    1986: 6,\n",
    "    1987: 6,\n",
    "    1988: 6\n",
    "}\n",
    "kde_bandwidth = 2.0\n",
    "df_counts_irr = add_counts_from_dict(df_counts_irr, year_count_dict)\n",
    "kde_irr = calculate_kde(df_counts_irr, bandwidth=kde_bandwidth)\n",
    "histogram_plot(\n",
    "    df_counts_irr,\n",
    "    title='Histogram of irrigation reservoir construction dates',\n",
    "    bandwidth=kde_bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating multiple construction scenarios for hydroelectric, multipurpose and irrigation reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Scenario:\n",
    "    \"\"\" \"\"\"\n",
    "    data: Dict[ScenarioName, Dict[ReservoirName, float]]\n",
    "    years_irr: List[int]\n",
    "    years_nonirr: List[int]\n",
    "\n",
    "@dataclass\n",
    "class ScenarioGenerator:\n",
    "    \"\"\" \"\"\"\n",
    "    input_file: str | Path\n",
    "    tabulardata_file: str | Path\n",
    "    irrigation_years_kde: KDEOutput\n",
    "    number_scenarios: int = 1_0000\n",
    "    input_data: Dict[str, Any] = field(init=False)\n",
    "    tabulardata_df: pd.DataFrame = field(init=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_json_to_dict(json_path: str | Path) -> Dict[str, Any]:\n",
    "        \"\"\"Load a JSON file and return its contents as a dictionary.\"\"\"\n",
    "        json_path = Path(json_path)\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_csv_to_df(csv_path: str | Path, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a CSV file and return its contents as a pandas DataFrame.\n",
    "        Additional keyword arguments are passed to pd.read_csv.\n",
    "        \"\"\"\n",
    "        csv_path = Path(csv_path)\n",
    "        return pd.read_csv(csv_path, **kwargs)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\" \"\"\"\n",
    "        self.input_data = self.load_json_to_dict(self.input_file)\n",
    "        self.tabulardata_df = self.load_csv_to_df(self.tabulardata_file)\n",
    "        reservoir_names_csv = list(self.tabulardata_df['name'])\n",
    "        reservoir_names_json = list(self.input_data.keys())\n",
    "        # Check if both data structures contain the same reservoirs\n",
    "        missing_in_json = set(reservoir_names_csv) - set(reservoir_names_json)\n",
    "        missing_in_csv = set(reservoir_names_json) - set(reservoir_names_csv)\n",
    "        if missing_in_json or missing_in_csv:\n",
    "            raise ValueError(\n",
    "                \"Reservoir names mismatch between JSON and CSV files.\",\n",
    "                f\"Reservoirs in CSV but missing in JSON: {missing_in_json}\",\n",
    "                f\"Reservoirs in JSON but missing in CSV: {missing_in_csv}\")\n",
    "            \n",
    "    def get_projections(self, random_state: int = 42) -> Scenario:\n",
    "        # Construct a mapping between reseroir names and their construction years for reservoirs with known construction years\n",
    "        reservoir_years_known = {\n",
    "            row['name']: int(row['construction_year'])\n",
    "            for _, row in self.tabulardata_df.iterrows()\n",
    "            if pd.notna(row['construction_year']) and row['construction_year'] > 0\n",
    "        }\n",
    "        irr_reservoirs = self.tabulardata_df.query(\"type == 'irrigation'\")['name'].tolist()\n",
    "        nonirr_reservoirs = self.tabulardata_df.query(\"type != 'irrigation'\")['name'].tolist()\n",
    "        irr_reservoirs_unknown = list(set(irr_reservoirs) - set(reservoir_years_known.keys()))\n",
    "        nonirr_reservoirs_unknown = list(set(nonirr_reservoirs) - set(reservoir_years_known.keys()))\n",
    "        # Generate years for irrigation reservoir with unknown construction years\n",
    "        num_irr_unknown = len(irr_reservoirs_unknown)\n",
    "        num_nonirr_unknown = len(nonirr_reservoirs_unknown)\n",
    "        irr_years_rnd = np.array(sample_years_kernel(\n",
    "            kde=self.irrigation_years_kde.kde,\n",
    "            n_samples=num_irr_unknown * self.number_scenarios,\n",
    "            random_state=random_state))\n",
    "        nonirr_years_rnd = np.array(sample_years_uniform(\n",
    "            min_year=2025, max_year=2045,\n",
    "            n_samples=num_nonirr_unknown * self.number_scenarios,\n",
    "            random_state=random_state))\n",
    "        # Create the scenario data\n",
    "        chunks_irr = irr_years_rnd.reshape(self.number_scenarios, -1)  # shape: (number_scenarios, num_irr_unknown)\n",
    "        chunks_nonirr = nonirr_years_rnd.reshape(self.number_scenarios, -1)  # shape: (number_scenarios, num_irr_unknown)   \n",
    "        scenario_dicts_irr = [dict(zip(irr_reservoirs_unknown, map(int, chunk))) for chunk in chunks_irr]\n",
    "        scenario_dicts_nonirr = [dict(zip(nonirr_reservoirs_unknown, map(int, chunk))) for chunk in chunks_nonirr]\n",
    "        scenario_data = {\n",
    "            sc_index: reservoir_years_known | scenario_dicts_irr[sc_index] | scenario_dicts_nonirr[sc_index]\n",
    "            for sc_index in range(self.number_scenarios)\n",
    "        }\n",
    "        #return scenario_data\n",
    "        return Scenario(\n",
    "            data=scenario_data,\n",
    "            years_irr=irr_years_rnd,\n",
    "            years_nonirr=nonirr_years_rnd\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenarios using the defined ScenarioGenerator class\n",
    "generator = ScenarioGenerator(\n",
    "    input_file=input_file, \n",
    "    tabulardata_file=construction_years,\n",
    "    irrigation_years_kde=kde_irr)\n",
    "scenario = generator.get_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check probability distributions of the generated construction years\n",
    "#### -- They should visually match the distributions they were sampled from --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot_simple(\n",
    "    scenario.years_irr.flatten(), \n",
    "    title='Histogram of sampled irrigation reservoir construction years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot_simple(\n",
    "    scenario.years_nonirr.flatten(), \n",
    "    title='Histogram of sampled hydroelectric and multipurpose reservoir construction years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate emissions and generate emission profiles for each scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate emissions with ReEmission\n",
    "input_data = Inputs.fromfile(input_file) \n",
    "model = EmissionModel(inputs=input_data, p_model='g-res')\n",
    "model.calculate()\n",
    "simulation_outputs = model.outputs\n",
    "# Calculate and save emission profiles\n",
    "profiles_per_scenario: Dict[ReservoirName, EmissionProfileSeries] = dict()\n",
    "year_vector = input_data.inputs[list(input_data.inputs.keys())[0]].data['year_vector']\n",
    "# Define units for the area and aerial emissions represented in emission profiles\n",
    "unit_area = ureg('km**2')\n",
    "unit_profile = ureg('g_CO2e_per_metre2_per_year') # registered unit for emission profiles in emission_profile.py\n",
    "# Precompute the emission profiles for each reservoir\n",
    "em_vectors_per_reservoir: Dict[ReservoirName, EmissionProfileSeries] = dict()\n",
    "res_types: Dict[ReservoirName, str] = dict()\n",
    "for res_name, res_data in simulation_outputs.items():\n",
    "    # Get the emission profile for the reservoir\n",
    "    area = input_data.inputs[res_name].data['reservoir']['area'] * unit_area\n",
    "    areal_profile_with_unit = \\\n",
    "        np.array(res_data['co2_profile'], dtype=np.float32) * unit_profile + \\\n",
    "        np.array(res_data['ch4_profile'], dtype=np.float32) * unit_profile\n",
    "    tot_profile = (areal_profile_with_unit * area).to('Mt_CO2e / year')\n",
    "    # Create an EmissionProfile object\n",
    "    unit_conv = str(tot_profile.units)\n",
    "    em = [Emission(value, unit_conv) for value in tot_profile.magnitude]\n",
    "    em_vectors_per_reservoir[res_name] = em\n",
    "    res_types[res_name] = input_data.inputs[res_name].data['type']\n",
    "\n",
    "def build_profile_for_scenario(\n",
    "    sc_ix: int,\n",
    "    sc: Dict[str, Union[int, float]],\n",
    "    year_vector: List[int],\n",
    "    em_vectors: Dict[str, List[Emission]]) -> Optional[Tuple[int, EmissionProfileSeries]]:\n",
    "    \"\"\" \"\"\"\n",
    "    profiles_irr = []\n",
    "    profiles_nonirr = []\n",
    "    for res_name, construction_date in sc.items():\n",
    "        em = em_vectors.get(res_name)\n",
    "        if em is None or construction_date is None:\n",
    "            continue\n",
    "        em_profile = EmissionProfile(values=em, years=year_vector).to_series(construction_date)\n",
    "        if res_types[res_name] == 'irrigation':\n",
    "            profiles_irr.append(em_profile)\n",
    "        else:\n",
    "            profiles_nonirr.append(em_profile)\n",
    "    if profiles_irr and profiles_nonirr:\n",
    "        return sc_ix, EmissionProfileSeries.combine(profiles_irr), EmissionProfileSeries.combine(profiles_nonirr)\n",
    "    return None\n",
    "\n",
    "out_path = Path(\"../outputs/profiles_per_scenario.pkl\")\n",
    "if out_path.exists():\n",
    "    with out_path.open(\"rb\") as f:\n",
    "        profiles_per_scenario = pickle.load(f)\n",
    "else:\n",
    "    # Container for the results\n",
    "    profiles_per_scenario: Dict[int, 'EmissionProfileSeries'] = {}\n",
    "    # Submit parallel tasks\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [\n",
    "            executor.submit(build_profile_for_scenario, sc_ix, sc, year_vector, em_vectors_per_reservoir)\n",
    "            for sc_ix, sc in scenario.data.items()\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                sc_ix, profile_series_irr, profile_series_nonirr = result\n",
    "                profiles_per_scenario[sc_ix] = {}\n",
    "                profiles_per_scenario[sc_ix]['irrigation'] = profile_series_irr\n",
    "                profiles_per_scenario[sc_ix]['nonirrigation'] = profile_series_nonirr\n",
    "    # dump to disk\n",
    "    with out_path.open(\"wb\") as f:\n",
    "        pickle.dump(profiles_per_scenario, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the emission profile plot for the publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "def make_publication_plot(\n",
    "    profiles: Dict[str, Dict[str, EmissionProfileSeries]] = profiles_per_scenario,\n",
    "    n_series: int | None = 10,\n",
    "    filename: Path = Path('../outputs/emission_profile.svg')) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a publication-ready plot for all emission projections in profiles_per_scenario.\n",
    "    \"\"\"\n",
    "    # 1) Set overall style to serif, tweak font sizes, remove grid, etc.\n",
    "    plt.rcParams.update({\n",
    "        'font.family':       'serif',\n",
    "        'font.serif':        ['Times New Roman', 'Times', 'Palatino'],\n",
    "        'font.size':         12,\n",
    "        'axes.titlesize':    14,\n",
    "        'axes.labelsize':    13,\n",
    "        'xtick.labelsize':   11,\n",
    "        'ytick.labelsize':   11,\n",
    "        'legend.fontsize':   11,\n",
    "        'axes.linewidth':    1.0,\n",
    "        'xtick.major.width': 0.8,\n",
    "        'ytick.major.width': 0.8,\n",
    "        'xtick.minor.width': 0.6,\n",
    "        'ytick.minor.width': 0.6,\n",
    "        'xtick.direction':   'in',\n",
    "        'ytick.direction':   'in',\n",
    "        'xtick.bottom':         True,\n",
    "        'ytick.left':       True,\n",
    "        'xtick.top':         False,\n",
    "        'ytick.right':       False,\n",
    "        'figure.facecolor':  'white',\n",
    "        'axes.facecolor':    'white',\n",
    "        'axes.grid':         False,\n",
    "    })\n",
    "    \n",
    "    # 2) Collect the series in lists, plot thin lines as before\n",
    "    irr_list   = []\n",
    "    total_list = []\n",
    "    # Prepare for peak collection\n",
    "    peak_records = []\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 3.75))\n",
    "\n",
    "    # Loop through each scenario\n",
    "    for iter, (scenario_id, profiles) in enumerate(profiles.items()):\n",
    "        if n_series is not None and iter >= n_series:\n",
    "            break\n",
    "        # Extract the raw pd.Series of Emission objects\n",
    "        irr_series = profiles['irrigation'].values       # pd.Series of Emission\n",
    "        nonirr_series = profiles['nonirrigation'].values # pd.Series of Emission\n",
    "        # Convert to floats\n",
    "        irr_vals = irr_series.map(lambda em: em.value)\n",
    "        nonirr_vals = nonirr_series.map(lambda em: em.value)\n",
    "        total_vals = irr_vals + nonirr_vals\n",
    "        \n",
    "        peak_val  = total_vals.max()\n",
    "        peak_time = total_vals.idxmax()\n",
    "        peak_records.append({\n",
    "            'scenario_id': scenario_id,\n",
    "            'peak_val':    peak_val,\n",
    "            'peak_time':   peak_time\n",
    "        })\n",
    "        \n",
    "        # Rename each Series so that concat knows their column name\n",
    "        irr_list.append(irr_vals.rename(scenario_id))\n",
    "        total_list.append(total_vals.rename(scenario_id))\n",
    "\n",
    "        # Plot irrigation alone\n",
    "        ax.plot(\n",
    "            irr_vals.index,\n",
    "            irr_vals.values,\n",
    "            linewidth=0.3,      # very thin line\n",
    "            alpha=0.02,          # semi‐transparent\n",
    "            color='tab:blue'\n",
    "        )\n",
    "\n",
    "        # Plot irrigation + nonirrigation\n",
    "        ax.plot(\n",
    "            total_vals.index,\n",
    "            total_vals.values,\n",
    "            linewidth=0.3,\n",
    "            alpha=0.02,\n",
    "            color='tab:orange'\n",
    "        )\n",
    "    \n",
    "    # 3) Build DataFrames & medians\n",
    "    # Now concatenate all at once (much faster, no fragmentation warnings)\n",
    "    irr_df   = pd.concat(irr_list,   axis=1)\n",
    "    total_df = pd.concat(total_list, axis=1)\n",
    "    # Compute medians\n",
    "    median_irr = irr_df.median(axis=1)\n",
    "    median_total = total_df.median(axis=1)\n",
    "    # Align onto the intersection of their indices so that x, y1, y2 have equal length\n",
    "    common_index = median_irr.index.intersection(median_total.index)\n",
    "    median_irr   = median_irr.loc[common_index]\n",
    "    median_total = median_total.loc[common_index]\n",
    "        \n",
    "    # 4) Shade and overplot medians\n",
    "    ax.fill_between(\n",
    "        common_index, \n",
    "        0, \n",
    "        median_irr.values,\n",
    "        color='lightblue',\n",
    "        alpha=0.5,\n",
    "        label='Cumulative emissions from IRR resservoirs'\n",
    "    )\n",
    "    # Shade between median irrigation and total\n",
    "    ax.fill_between(\n",
    "        common_index,\n",
    "        median_irr.values,\n",
    "        median_total.values,\n",
    "        color='navajowhite',  # light orangeEmiss\n",
    "        alpha=0.5,\n",
    "        label='Cumulative emissions from HP and MP resservoirs'\n",
    "    )\n",
    "    ax.plot([], [], color='tab:blue', label='IRR')\n",
    "    ax.plot([], [], color='tab:orange', label='IRR + HP + MP')\n",
    "    # Plot median lines\n",
    "    ax.plot(common_index, median_irr.values,\n",
    "            color='grey', linewidth=2, label='Median emissions')\n",
    "    ax.plot(common_index, median_total.values,\n",
    "            color='grey', linewidth=2)\n",
    "    \n",
    "    # 5) Compute peaks and add them to the plot\n",
    "    peaks_df = pd.DataFrame(peak_records)\n",
    "\n",
    "    sorted_df = peaks_df.sort_values('peak_val').reset_index(drop=True)\n",
    "    mid_idx = len(sorted_df) // 2\n",
    "    med_row = sorted_df.iloc[mid_idx]\n",
    "    \n",
    "    # Global max‐peak\n",
    "    max_row = peaks_df.loc[peaks_df['peak_val'].idxmax()]\n",
    "    # Global min‐peak\n",
    "    min_row = peaks_df.loc[peaks_df['peak_val'].idxmin()]\n",
    "    # Mark total max, min, and median peak scenarios\n",
    "    ax.scatter(max_row['peak_time'], max_row['peak_val'], linewidth=0.4,\n",
    "               color='tab:orange', marker='o', s=30, zorder=5, alpha=0.6, edgecolor='black', \n",
    "               label=\"Maximum peak emission\")\n",
    "    ax.scatter(min_row['peak_time'], min_row['peak_val'], linewidth=0.4,\n",
    "               color='tab:blue', marker='o', s=30, zorder=5, alpha=0.6, edgecolor='black', \n",
    "               label=\"Minimum peak emission\")\n",
    "    # NEW: annotate median peak scenario\n",
    "    ax.scatter(med_row['peak_time'], med_row['peak_val'], linewidth=0.4,\n",
    "               color='tab:green', marker='o', s=30, zorder=5, alpha=0.6, edgecolor='black', \n",
    "               label=(\"Median peak emission\"))\n",
    "\n",
    "    # 6) Axes formatting\n",
    "    ax.set_xlim(pd.Timestamp('1960-01-01'), common_index.max())\n",
    "    ax.set_ylim(bottom=0)\n",
    "    # Tight ticks, minor ticks on\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    # Labels & title\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('GHG Emissions (Mt CO2e / year)')\n",
    "    #plt.title('Irrigation vs. Total Emission Profiles (all scenarios)')\n",
    "\n",
    "    # (Optional) add a legend entry manually if you like\n",
    "    ax.legend(frameon=False, loc='upper left')\n",
    "    fig.tight_layout()\n",
    "    if filename:\n",
    "        fig.savefig(filename, format='svg', dpi=300)\n",
    "    plt.show()\n",
    "    return peaks_df\n",
    "    \n",
    "make_publication_plot(n_series=1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reemission_deploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
